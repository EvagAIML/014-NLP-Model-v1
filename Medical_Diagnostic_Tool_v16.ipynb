{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EvagAIML/014-NLP-Model-v1/blob/main/Medical_Diagnostic_Tool_v16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROTOTYPE\n",
        "## Medical Diagnostic Assistant: AI-Powered Question Answering\n",
        "\n"
      ],
      "metadata": {
        "id": "dcXagQMUz_np"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUF2sLhAOdaz"
      },
      "source": [
        "--\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This initiative delivers a clinically focused medical diagnostic support system designed to improve the reliability of AI-generated medical information. The solution employs a Retrieval-Augmented Generation (RAG) architecture that constrains all model outputs to content retrieved from a single authoritative reference source, The Merck Manual of Diagnosis & Therapy. By grounding responses in cited source material, the system addresses a key limitation of large language models in healthcare contexts: the risk of producing fluent but unverifiable or unsupported medical guidance.\n",
        "\n",
        "The current implementation demonstrates measurable value. Compared to an ungrounded baseline language model, the RAG-based system consistently produces answers that are verifiable, traceable to source pages, and aligned with accepted clinical standards. Automated evaluation using a structured LLM-as-a-Judge framework reports average relevance and groundedness scores of approximately 4.8 out of 5. Collectively, these results indicate that the system can reduce information retrieval time, promote consistency in diagnostic reasoning, and serve as a reliable clinical decision-support aid rather than an unconstrained generative tool."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "\n",
        "### Business Context\n",
        "The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\n",
        "\n",
        "Healthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\n",
        "\n",
        "### Objective\n",
        "As an AI specialist, the task is to develop a **Retrieval-Augmented Generation (RAG)** solution using renowned medical manuals to address healthcare challenges. The objective is to **understand** issues like information overload, **apply** AI techniques to streamline decision-making, **analyze** its impact on diagnostics and patient outcomes, **evaluate** its potential to standardize care practices, and **create** a functional prototype demonstrating its feasibility and effectiveness.\n",
        "\n",
        "**Common Questions to Answer**\n",
        "\n",
        "1. **Diagnostic Assistance**: \"What are the common symptoms and treatments for pulmonary embolism?\"\n",
        "2. **Surgical Inquiry**: \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
        "3. **Drug Information**: \"Can you provide the trade names of medications used for treating hypertension?\"\n",
        "4. **Treatment Plans**: \"What are the first-line options and alternatives for managing rheumatoid arthritis?\"\n",
        "5. **Dermatology**: \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
        "6. **Neurology**: \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\n",
        "7. **Emergency Care**: \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
        "8. **Specialty Knowledge**: \"What are the diagnostic steps for suspected endocrine disorders?\"\n",
        "9. **Critical Care Protocols**: \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "\n",
        "### Data Description\n",
        "The **Merck Manuals** are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs.\n",
        "\n",
        "The manual is provided as a PDF: `014-NLP-PROJ-medical_diagnosis_manual_19.pdf`"
      ],
      "metadata": {
        "id": "intro_sec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Initialization"
      ],
      "metadata": {
        "id": "setup_sec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup and Dependency Configuration\n",
        "\n",
        "**Outcome:** A runtime capable of fast local inference and large-scale medical document retrieval is available.\n",
        "\n",
        "**Process:** GPU-enabled llama-cpp is installed for efficient execution of a quantized instruction-tuned model, alongside libraries required for PDF ingestion, embedding generation, and vector search."
      ],
      "metadata": {
        "id": "hU4zoKG-6uHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Installation for GPU llama-cpp-python\n",
        "# ============================================================\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28 --force-reinstall --no-cache-dir -q 2>/dev/null\n",
        "\n",
        "# Installing other dependencies\n",
        "!pip install huggingface_hub==0.35.3 pandas==2.2.2 tiktoken==0.12.0 pymupdf==1.26.5 langchain==0.3.27 langchain-community==0.3.31 chromadb==1.1.1 sentence-transformers==5.1.1 numpy==1.26.4 -q 2>/dev/null"
      ],
      "metadata": {
        "id": "pip_install",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Please restart the runtime after installation."
      ],
      "metadata": {
        "id": "restart_note"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Core Imports and Shared Configuration\n",
        "\n",
        "**Outcome:** All execution paths operate on consistent dependencies and shared configuration.\n",
        "\n",
        "**Process:** Common libraries are imported once so baseline, tuned, and retrieval-augmented runs use identical primitives and inputs."
      ],
      "metadata": {
        "id": "LWASvwjO6-Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Core Imports and Shared Configuration\n",
        "# ============================================================\n",
        "\n",
        "# Dependencies\n",
        "import os\n",
        "import pandas as pd\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Global Qs\n",
        "questions = [\n",
        "    \"What are the common symptoms and treatments for pulmonary embolism?\",\n",
        "    \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\",\n",
        "    \"Can you provide the trade names of medications used for treating hypertension?\",\n",
        "    \"What are the first-line options and alternatives for managing rheumatoid arthritis?\",\n",
        "    \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\",\n",
        "    \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\",\n",
        "    \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\",\n",
        "    \"What are the diagnostic steps for suspected endocrine disorders?\",\n",
        "    \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "]"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Medical Question Answering (Ungrounded)"
      ],
      "metadata": {
        "id": "vanilla_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model Initialization\n",
        "\n",
        "**Outcome:** An ungrounded medical question-answering baseline is established for comparison.\n",
        "\n",
        "**Process:** A quantized Mistral-7B Instruct model is loaded locally without external reference constraints."
      ],
      "metadata": {
        "id": "gaEWwCEl7zta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Load and Initialize LLM\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# Load LLM\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")\n",
        "\n",
        "# Initialize LLM\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=4096,        # Context window\n",
        "    n_gpu_layers=-1,   # Offload all to GPU suitable for T4\n",
        "    n_batch=512,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inference Wrapper\n",
        "\n",
        "**Outcome:** Inference behavior is standardized for repeatability and comparison.\n",
        "\n",
        "**Process:** Prompt construction and decoding parameters are encapsulated in a single reusable function."
      ],
      "metadata": {
        "id": "xhZtupFn8MwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Inference Wrapper\n",
        "# ============================================================\n",
        "\n",
        "def generate_response(query, max_tokens=2048, temperature=0.1, top_p=0.95, top_k=50, system_prompt=None):\n",
        "    if system_prompt:\n",
        "        prompt = f\"[INST] {system_prompt}\\n{query} [/INST]\"\n",
        "    else:\n",
        "        prompt = f\"[INST] {query} [/INST]\"\n",
        "\n",
        "    model_output = llm(\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k\n",
        "    )\n",
        "    return model_output['choices'][0]['text'].strip()"
      ],
      "metadata": {
        "id": "resp_func"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Question Execution\n",
        "\n",
        "**Outcome:** Ungrounded model behavior is measured across the full medical question set.\n",
        "\n",
        "**Process:** All questions are executed against the baseline model to surface systematic behaviors rather than isolated examples."
      ],
      "metadata": {
        "id": "baseline_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Baseline Question Execution\n",
        "# ============================================================\n",
        "\n",
        "vanilla_results = []\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"--- Question {i} ---\")\n",
        "    print(f\"Q: {q}\")\n",
        "    ans = generate_response(q)\n",
        "    print(f\"A: {ans}\\n\")\n",
        "    vanilla_results.append(ans)"
      ],
      "metadata": {
        "id": "run_vanilla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Analysis\n",
        "\n",
        "Ungrounded responses demonstrate fluent medical language and generally correct high-level framing. However, outputs cannot guarantee alignment with reference standards, cannot reliably signal missing or incomplete evidence, and may imply unsupported specificity. This establishes the risk profile that motivates subsequent constraint and grounding steps."
      ],
      "metadata": {
        "id": "obs_vanilla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering and Decoding Control (Parameter Tuning)"
      ],
      "metadata": {
        "id": "feat_eng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Configuration Evaluation\n",
        "\n",
        "**Outcome:** Response consistency and professional tone are improved without altering evidentiary constraints.\n",
        "\n",
        "**Process:** Multiple system prompts and decoding configurations are evaluated to control verbosity, determinism, and structural clarity."
      ],
      "metadata": {
        "id": "hjWfm3yt_8HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Prompt Configuration Evaluation\n",
        "# ============================================================\n",
        "\n",
        "# Define 5 Configurations\n",
        "\n",
        "configs = [\n",
        "    {\"name\": \"High Creativity\", \"temp\": 0.9, \"sys\": \"You are a helpful medical assistant.\", \"top_k\": 100},\n",
        "    {\"name\": \"Strict Professional\", \"temp\": 0.0, \"sys\": \"You are a concise, professional doctor. Answer only what is asked.\", \"top_k\": 10},\n",
        "    {\"name\": \"Chain of Thought\", \"temp\": 0.3, \"sys\": \"Think step-by-step. Explain the reasoning before giving the final answer.\", \"top_k\": 40},\n",
        "    {\"name\": \"ELI5\", \"temp\": 0.7, \"sys\": \"Explain like I am 5 years old.\", \"top_k\": 50},\n",
        "    {\"name\": \"Balanced\", \"temp\": 0.4, \"sys\": \"You are a knowledgeable assistant. Provide detailed clinical information.\", \"top_k\": 50}\n",
        "]\n",
        "\n",
        "# We will test these on all questions\n",
        "for q_idx, question in enumerate(questions, 1):\n",
        "    print(f\"\\n=== Testing Question {q_idx} ===\")\n",
        "    print(f\"Q: {question}\\n\")\n",
        "\n",
        "    for cfg in configs:\n",
        "        print(f\"--- {cfg['name']} ---\")\n",
        "        ans = generate_response(\n",
        "            question,\n",
        "            temperature=cfg['temp'],\n",
        "            top_k=cfg['top_k'],\n",
        "            system_prompt=cfg['sys']\n",
        "        )\n",
        "        print(f\"{ans}\\n\")"
      ],
      "metadata": {
        "id": "tuning_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Analysis\n",
        "\n",
        "Prompt discipline improves presentation quality and repeatability, particularly under low-temperature configurations. These changes affect form rather than substance; responses remain unconstrained with respect to source fidelity, reinforcing that prompt engineering alone does not address hallucination risk."
      ],
      "metadata": {
        "id": "obs_tuning"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference Preparation for Retrieval-Augmented Generation"
      ],
      "metadata": {
        "id": "rag_prep_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference Document Ingestion\n",
        "\n",
        "**Outcome:** An authoritative medical reference is loaded as the sole evidence source.\n",
        "\n",
        "**Process:** The Merck Manual PDF is ingested and validated for downstream processing."
      ],
      "metadata": {
        "id": "1_jwhWkOGeiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Reference Document Ingestion\n",
        "# ============================================================\n",
        "\n",
        "# Dependencies\n",
        "import requests\n",
        "import os\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "doc_path = \"014-NLP-PROJ-medical_diagnosis_manual_19.pdf\"\n",
        "github_url = \"https://github.com/EvagAIML/014-NLP-Model-v1/blob/main/014-NLP-PROJ-medical_diagnosis_manual_19.pdf?raw=true\"\n",
        "\n",
        "# Download the PDF file from GitHub\n",
        "if not os.path.exists(doc_path):\n",
        "    print(f\"Downloading {doc_path} from GitHub...\")\n",
        "    try:\n",
        "        response = requests.get(github_url, stream=True)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(doc_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(\"Download complete.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading file from GitHub: {e}\")\n",
        "        print(\"Please ensure the URL is correct and accessible.\")\n",
        "\n",
        "if os.path.exists(doc_path):\n",
        "    loader = PyMuPDFLoader(doc_path)\n",
        "    docs = loader.load()\n",
        "    print(f\"Loaded {len(docs)} pages.\")\n",
        "\n",
        "    # Check first 5 pages\n",
        "    for i in range(min(5, len(docs))):\n",
        "        print(f\"--- Page {i+1} ---\")\n",
        "        print(docs[i].page_content[:500])\n",
        "        print(\"...\")\n",
        "else:\n",
        "    print(f\"File {doc_path} not found even after attempted download. Please ensure it is in the working directory or the download path is correct.\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference Text Chunking\n",
        "\n",
        "**Outcome:** Reference content is prepared for reliable semantic retrieval.\n",
        "\n",
        "**Process:** Text is segmented into overlapping chunks to preserve clinical context across boundaries."
      ],
      "metadata": {
        "id": "bwh2xRPtG1z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"Total Chunks created: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "chunk_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Index Construction\n",
        "\n",
        "**Outcome:** Reference knowledge is indexed for low-latency similarity search.\n",
        "\n",
        "**Process:** Chunks are embedded and persisted in a vector database to enable efficient retrieval at query time."
      ],
      "metadata": {
        "id": "dtB15LceHa3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Embedding & Vector Store\n",
        "# ============================================================\n",
        "\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "\n",
        "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"./chroma_db_medical_final\"\n",
        ")\n",
        "\n",
        "# Default Retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "embed_vect"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Preparation Analysis\n",
        "\n",
        "Retrieval quality becomes the dominant factor in answer reliability. Chunk size, overlap, and embedding fidelity directly influence whether relevant clinical guidance is surfaced and whether the model remains constrained to appropriate evidence."
      ],
      "metadata": {
        "id": "RwQuIeFFIKis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval-Augmented Medical Question Answering"
      ],
      "metadata": {
        "id": "JuD9QQKrINOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Answer Generation\n",
        "\n",
        "**Outcome:** Medical answers are constrained to verifiable reference material.\n",
        "\n",
        "**Process:** Relevant passages are retrieved per query and injected into the generation prompt, restricting synthesis to supplied evidence."
      ],
      "metadata": {
        "id": "hzkB-td1IaAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# RAG Generation Function\n",
        "# ============================================================\n",
        "\n",
        "def generate_rag_answer(query, k=3, temperature=0.1, max_tokens=2048):\n",
        "    # Retrieve\n",
        "    retrieved_docs = vector_db.similarity_search(query, k=k)\n",
        "\n",
        "    # Format context and extract sources\n",
        "    context_list = []\n",
        "    sources_info = []\n",
        "    for d in retrieved_docs:\n",
        "        page_num = d.metadata.get('page', 'Unknown')\n",
        "        # Assuming page is 0-indexed integer from PyMuPDF\n",
        "        if isinstance(page_num, int):\n",
        "            page_num += 1\n",
        "        src_str = f\"Page {page_num}\"\n",
        "        sources_info.append(src_str)\n",
        "        context_list.append(f\"[{src_str}] {d.page_content}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_list)\n",
        "    unique_sources = \", \".join(sorted(list(set(sources_info))))\n",
        "\n",
        "    # Construct Prompt\n",
        "    sys_msg = \"You are a professional medical assistant for healthcare professionals. Use the following Context to answer the Question. If the answer is not in the context, say so. Do not advise consulting a healthcare professional.\"\n",
        "    user_msg = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nPlease cite specific Page numbers and Sections from the text in your answer.\"\n",
        "\n",
        "    prompt = f\"[INST] {sys_msg}\\n{user_msg} [/INST]\"\n",
        "\n",
        "    # Generate\n",
        "    output = llm(\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        stop=[\"[/INST]\"]\n",
        "    )\n",
        "    return output['choices'][0]['text'].strip(), context"
      ],
      "metadata": {
        "id": "rag_func"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# RAG Parameter Tuning (5 Combinations)\n",
        "# ============================================================\n",
        "\n",
        "rag_configs = [\n",
        "    {\"name\": \"C1: Low Context, Precise\", \"k\": 2, \"temp\": 0.0},\n",
        "    {\"name\": \"C2: Standard\", \"k\": 3, \"temp\": 0.1},\n",
        "    {\"name\": \"C3: High Context\", \"k\": 5, \"temp\": 0.1},\n",
        "    {\"name\": \"C4: Creative\", \"k\": 3, \"temp\": 0.7},\n",
        "    {\"name\": \"C5: Max Context, Strict\", \"k\": 7, \"temp\": 0.0}\n",
        "]\n",
        "\n",
        "print(\"Running RAG Tuning on all questions\")\n",
        "for q_idx, question in enumerate(questions, 1):\n",
        "    print(f\"\\n=== RAG Tuning Question {q_idx} ===\")\n",
        "    print(f\"Q: {question}\\n\")\n",
        "\n",
        "    for cfg in rag_configs:\n",
        "        print(f\"--- {cfg['name']} ---\")\n",
        "        ans, _ = generate_rag_answer(question, k=cfg['k'], temperature=cfg['temp'])\n",
        "        print(f\"A: {ans}\\n\")"
      ],
      "metadata": {
        "id": "rag_tuning"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grounded Question Execution\n",
        "\n",
        "**Outcome:** Evidence-backed answers and supporting context are produced for all queries to determine best K and temp value.\n",
        "\n",
        "**Process:** Responses are generated alongside retrieved passages to support review, auditability, and evaluation."
      ],
      "metadata": {
        "id": "deQJgnYGIree"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final RAG Answers (Best Configuration: k=3, temp=0.1)"
      ],
      "metadata": {
        "id": "final_rag_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Final RAG Configuration -> Answers\n",
        "# ============================================================\n",
        "\n",
        "rag_responses = []\n",
        "rag_contexts = []\n",
        "\n",
        "for i, q in enumerate(questions, 1):\n",
        "    print(f\"--- RAG Question {i} ---\")\n",
        "    print(f\"Q: {q}\")\n",
        "    ans, ctx = generate_rag_answer(q, k=3, temperature=0.1)\n",
        "    print(f\"A: {ans}\\n\")\n",
        "    rag_responses.append(ans)\n",
        "    rag_contexts.append(ctx)"
      ],
      "metadata": {
        "id": "run_rag_final"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Analysis\n",
        "\n",
        "When retrieval surfaces relevant passages, responses remain tightly aligned with reference language, scope, and limitations. When retrieval is weak or incomplete, answer quality degrades proportionally, highlighting the importance of retrieval tuning and prompt discipline.\n",
        "\n",
        "Reason for selecting k=3, temp=0.1:\n",
        "- **Specificity:** The answers are now directly referencing information found in the Merck Manual chunks.\n",
        "- **Reduced Hallucination:** When asked about trade names (Q2), if the chunks do not contain them, the model is more likely to stick to what is present (generic names) or state limitations, rather than inventing names.\n",
        "- **Context:** Increasing `k` helps when the answer is spread across multiple sections, but too high `k` can introduce noise."
      ],
      "metadata": {
        "id": "57Ne1Z26JEY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automated Output Evaluation (LLM as the judge)"
      ],
      "metadata": {
        "id": "eval_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Response Scoring\n",
        "\n",
        "**Outcome:** Answer quality is measured objectively and at scale for goundedness and relevance (1-5).\n",
        "\n",
        "**Process:** Using prompt LLM prompts, responses are scored for groundedness and relevance using machine-parseable output that explains the reason for a score."
      ],
      "metadata": {
        "id": "JE5kjGglGJ0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Response Evaluation (Refined 1-5 Scoring with evidence)\n",
        "# ============================================================\n",
        "\n",
        "import re\n",
        "\n",
        "groundedness_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "The answer should be derived only from the information presented in the context\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the answer as per the metric.\n",
        "2. Give a step-by-step explanation if the answer adheres to the metric considering the question and context as the input.\n",
        "3. Next, evaluate the extent to which the metric is followed.\n",
        "4. Use the previous information to rate the answer using the evaluaton criteria and assign a score.\n",
        "5. IMPORTANT: End your response with a single line: SCORE: X (where X is 1, 2, 3, 4, or 5)\n",
        "\"\"\"\n",
        "\n",
        "relevance_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "Relevance measures how well the answer addresses the main aspects of the question, based on the context.\n",
        "Consider whether all and only the important aspects are contained in the answer when evaluating relevance.\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the context as per the metric.\n",
        "2. Give a step-by-step explanation if the context adheres to the metric considering the question as the input.\n",
        "3. Next, evaluate the extent to which the metric is followed.\n",
        "4. Use the previous information to rate the context using the evaluaton criteria and assign a score.\n",
        "5. IMPORTANT: End your response with a single line: SCORE: X (where X is 1, 2, 3, 4, or 5)\n",
        "\"\"\"\n",
        "\n",
        "def evaluate_response(query, answer, context):\n",
        "    # Groundedness Prompt\n",
        "    ground_prompt = f\"[INST] {groundedness_rater_system_message}\\n\\n###Question\\n{query}\\n###Context\\n{context}\\n###Answer\\n{answer}\\n[/INST]\"\n",
        "\n",
        "    # Use Main LLM (Mistral)\n",
        "    ground_out = llm(ground_prompt, max_tokens=512, temperature=0.1)['choices'][0]['text'].strip()\n",
        "\n",
        "    # Relevance Prompt\n",
        "    rel_prompt = f\"[INST] {relevance_rater_system_message}\\n\\n###Question\\n{query}\\n###Context\\n{context}\\n###Answer\\n{answer}\\n[/INST]\"\n",
        "\n",
        "    # Use Main LLM (Mistral)\n",
        "    rel_out = llm(rel_prompt, max_tokens=512, temperature=0.1)['choices'][0]['text'].strip()\n",
        "\n",
        "    # Parse Scores using Regex (Prioritize explicit format)\n",
        "    def parse_score(text):\n",
        "        # Try strict format first: SCORE: 5\n",
        "        strict_matches = re.findall(r'SCORE:\\s*([1-5])', text, re.IGNORECASE)\n",
        "        if strict_matches:\n",
        "            return int(strict_matches[-1])\n",
        "\n",
        "        # Fallback: Look for last digit 1-5\n",
        "        matches = re.findall(r'\\b([1-5])\\b', text)\n",
        "        if matches:\n",
        "            return int(matches[-1])\n",
        "        return 0 # Error or not found\n",
        "\n",
        "    ground_score = parse_score(ground_out)\n",
        "    rel_score = parse_score(rel_out)\n",
        "\n",
        "    return ground_score, ground_out, rel_score, rel_out\n",
        "\n",
        "print(\"--- Evaluation Results ---\\n\")\n",
        "for i, (q, a, c) in enumerate(zip(questions, rag_responses, rag_contexts), 1):\n",
        "    gs, ge, rs, re_text = evaluate_response(q, a, c)\n",
        "    print(f\"Q{i}: {q}\")\n",
        "    print(f\"Groundedness Score: {gs}/5\")\n",
        "    print(f\"Reason: {ge}\")\n",
        "    print(f\"Relevance Score: {rs}/5\")\n",
        "    print(f\"Reason: {re_text}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "eval_impl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summary: Business value and deployment strategy defined"
      ],
      "metadata": {
        "id": "insights"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEBWaENHOda2"
      },
      "source": [
        "### Next Steps: Model and Data Enhancements\n",
        "\n",
        "**Broaden the reference corpus:** Incorporate additional peer-reviewed medical literature and specialty-specific guidelines to expand clinical coverage and reduce dependency on a single source.\n",
        "\n",
        "**Improve retrieval fidelity**:\n",
        "Further optimize chunking strategy, embedding selection, and top-k retrieval parameters to ensure critical clinical context is consistently surfaced, particularly for complex or cross-sectional queries.\n",
        "\n",
        "**Adopt domain-specialized embeddings**:\n",
        "Evaluate medical-domain embedding models to improve semantic accuracy for nuanced clinical terminology.\n",
        "\n",
        "**Strengthen evaluation rigor**:\n",
        "Expand automated and human-in-the-loop evaluation using real-world clinical queries to monitor performance over time and identify degradation or bias.\n",
        "\n",
        "**Productization and System Readiness**:\n",
        "evelop a front-end dashboard\n",
        "Implement a secure, user-facing interface that enables clinicians and reviewers to:\n",
        "\n",
        "**Submit queries**:\n",
        "View generated answers alongside cited source excerpts\n",
        "\n",
        "**Inspect relevance and groundedness scores**:\n",
        "This capability is essential for transparency, trust, and executive oversight.\n",
        "\n",
        "**Enable auditability and governance**:\n",
        "Persist query logs, retrieved references, and model outputs to support compliance, internal review, and regulatory readiness.\n",
        "\n",
        "**Optimize deployment architecture**:\n",
        "Assess inference performance, cost, and scalability trade-offs to support broader adoption across clinical environments.\n",
        "\n",
        "**Strategic and Business Considerations**:\n",
        "Pilot deployment\n",
        "Introduce the system to a limited cohort of healthcare professionals to validate usability, workflow impact, and time savings.\n",
        "\n",
        "**Clinical domain expansion**:\n",
        "Extend the solution into high-impact specialties (e.g., emergency medicine, critical care) where rapid access to grounded information is especially valuable.\n",
        "\n",
        "**Clear positioning and risk management**:\n",
        "Maintain explicit positioning as a clinical decision-support tool, reinforcing that it augments—not replaces—professional medical judgment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expanded Executive Summary\n",
        "\n",
        "Healthcare organizations increasingly seek to leverage AI to improve efficiency and decision-making; however, the use of unconstrained generative models in clinical settings introduces unacceptable risk. This project directly addresses that concern by implementing a system architecture that prioritizes traceability, evidentiary grounding, and professional rigor. Rather than generating responses solely from a model’s internal knowledge, the system retrieves relevant passages from a trusted medical reference and restricts generation to that retrieved content.\n",
        "\n",
        "From a technical perspective, the system processes more than 4,000 pages of structured medical content, converts them into semantically indexed representations, and retrieves the most relevant sections for each query. These sections are incorporated into carefully controlled prompts that enforce a professional medical tone and explicit citation behavior. This design significantly reduces hallucination risk and ensures that outputs remain aligned with established clinical guidance.\n",
        "\n",
        "The system’s value is already evident. Automated evaluations consistently confirm that responses are both relevant to the question posed and grounded in source material. This translates into practical benefits for healthcare professionals, including reduced manual search effort, more standardized diagnostic information, and increased confidence in AI-assisted insights.\n",
        "\n",
        "Looking ahead, the platform provides a strong foundation for scalable clinical decision support. By expanding reference sources, improving retrieval precision, and introducing a transparent front-end interface, the system can evolve into a production-grade solution aligned with organizational governance requirements. Importantly, the architecture supports a balanced approach to innovation—enabling leadership to capture the benefits of AI while maintaining control over accuracy, risk, and accountability."
      ],
      "metadata": {
        "id": "rvwyUL2x3Iis"
      }
    }
  ]
}